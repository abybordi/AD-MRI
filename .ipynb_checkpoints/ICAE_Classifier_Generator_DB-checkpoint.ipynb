{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('Tensorflow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Train/Test Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>age</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>scan_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OAS30001</th>\n",
       "      <td>OAS30001</td>\n",
       "      <td>65.0</td>\n",
       "      <td>CN</td>\n",
       "      <td>OAS30001_MR_d0129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OAS30002</th>\n",
       "      <td>OAS30002</td>\n",
       "      <td>68.0</td>\n",
       "      <td>CN</td>\n",
       "      <td>OAS30002_MR_d0371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OAS30003</th>\n",
       "      <td>OAS30003</td>\n",
       "      <td>60.0</td>\n",
       "      <td>CN</td>\n",
       "      <td>OAS30003_MR_d0558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OAS30004</th>\n",
       "      <td>OAS30004</td>\n",
       "      <td>58.0</td>\n",
       "      <td>CN</td>\n",
       "      <td>OAS30004_MR_d1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OAS30005</th>\n",
       "      <td>OAS30005</td>\n",
       "      <td>48.0</td>\n",
       "      <td>CN</td>\n",
       "      <td>OAS30005_MR_d0143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           subject   age diagnosis            scan_ID\n",
       "OAS30001  OAS30001  65.0        CN  OAS30001_MR_d0129\n",
       "OAS30002  OAS30002  68.0        CN  OAS30002_MR_d0371\n",
       "OAS30003  OAS30003  60.0        CN  OAS30003_MR_d0558\n",
       "OAS30004  OAS30004  58.0        CN  OAS30004_MR_d1101\n",
       "OAS30005  OAS30005  48.0        CN  OAS30005_MR_d0143"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ratio:  0.36862244897959184\n",
      "test_ratio:  0.3622448979591837\n"
     ]
    }
   ],
   "source": [
    "with open('AD_MRI_Master','rb') as f:\n",
    "    master_list = pickle.load(f)\n",
    "    \n",
    "master_sheet = pd.DataFrame.from_dict(master_list, orient='index')\n",
    "display(master_sheet.head())\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_list,test_list,train_label,test_label = train_test_split(master_sheet.subject.values,\n",
    "                                                               master_sheet.diagnosis.values\n",
    "                                                               ,random_state = 1337,test_size=0.2)\n",
    "\n",
    "\n",
    "AD_count = 0\n",
    "for lab in train_label:\n",
    "    if lab == 'AD':\n",
    "        AD_count +=1\n",
    "\n",
    "print('train_ratio: ',AD_count/len(train_label))\n",
    "\n",
    "AD_count = 0\n",
    "for lab in test_label:\n",
    "    if lab == 'AD':\n",
    "        AD_count +=1\n",
    "print('test_ratio: ',AD_count/len(test_label))\n",
    "#ratios are the same, we're good to go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.choice([(0,1),(0,2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom data generator.\n",
    "import random\n",
    "import scipy\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "\n",
    "import scipy\n",
    "\n",
    "def labels_to_categorical(labels):\n",
    "    le = LabelEncoder()\n",
    "    le.fit([\"CN\", \"AD\"])\n",
    "    num_lab=to_categorical(le.transform(labels),num_classes=2)\n",
    "    return num_lab\n",
    "\n",
    "\n",
    "\n",
    "class MyDataGenerator(Sequence):\n",
    "    \"\"\"Generates data for Keras\n",
    "    Sequence based data generator. Suitable for building data generator for training and prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, list_IDs, processed_dir,augment=False, \n",
    "                 to_fit=True, batch_size=32, dim=(120, 160, 120, 1),\n",
    "                 n_classes=2, shuffle=True):\n",
    "        \"\"\"Initialization\n",
    "        :param list_IDs: list of all 'label' PATHS\n",
    "        :param labels: list of image labels ***MUST ALREADY BE (N X 2) ARRAY***\n",
    "        :param image_path: path to images location\n",
    "        # \n",
    "        :param to_fit: True to return X and y, False to return X only\n",
    "        :param batch_size: batch size at each iteration\n",
    "        :param dim: tuple indicating image dimension\n",
    "        :param n_channels: number of image channels\n",
    "        :param n_classes: number of output masks\n",
    "        :param shuffle: True to shuffle label indexes after every epoch\n",
    "        \"\"\"\n",
    "        self.list_IDs = list_IDs\n",
    "        self.processed_dir = processed_dir\n",
    "        #self.labels = labels\n",
    "        self.augment = augment\n",
    "        #self.image_path = image_path not needed\n",
    "        #self.mask_path = mask_path   not needed\n",
    "        self.to_fit = to_fit\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        #self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\n",
    "        :return: number of batches per epoch\n",
    "        \"\"\"\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\n",
    "        :param index: index of the batch\n",
    "        :return: X and y when fitting. X only when predicting\n",
    "        \"\"\"\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        \n",
    "        # Generate data\n",
    "        X,y = self._load_data(list_IDs_temp)\n",
    "\n",
    "        if self.to_fit:\n",
    "           \n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def _load_data(self, list_IDs_temp):\n",
    "        \"\"\"Generates data containing batch_size images\n",
    "        :param list_IDs_temp: list of label ids to load\n",
    "        :return: batch of images\n",
    "        \"\"\"\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim))\n",
    "        y = np.empty((self.batch_size,self.n_classes), dtype=int)\n",
    "        \n",
    "        \n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            \n",
    "            #start X\n",
    "            with open(self.processed_dir+ID+'_data','rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                \n",
    "            \n",
    "            x_reshape= np.reshape(data['image'], (120, 160, 120, 1) )\n",
    "            if self.augment == True:\n",
    "                x_reshape = scipy.ndimage.rotate(x_reshape, \n",
    "                                           axes=random.choice([(0,1),(0,2),(1,2)]),\n",
    "                                           angle=random.choice([360-15,15]),\n",
    "                                           reshape=False)\n",
    "            X[i,] = x_reshape\n",
    "            \n",
    "            #start y\n",
    "            y[i,] = labels_to_categorical(data['diagnosis'])\n",
    "            \n",
    "            \n",
    "\n",
    "        return X,y\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '/Users/daniel/Desktop/PROCESSED/OAS30513_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-a199de83027b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmdg\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mMyDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprocessed_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmdg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-f1edcc20d542>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Generate data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_IDs_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_fit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-f1edcc20d542>\u001b[0m in \u001b[0;36m_load_data\u001b[0;34m(self, list_IDs_temp)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;31m#start X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/Users/daniel/Desktop/PROCESSED/OAS30513_data'"
     ]
    }
   ],
   "source": [
    "processed_dir = '/Users/daniel/Desktop/PROCESSED/'\n",
    "mdg= MyDataGenerator(test_list,processed_dir)\n",
    "\n",
    "mdg.__getitem__(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Desktop/PROCESSED\n"
     ]
    }
   ],
   "source": [
    "! ls /Users/daniel/Desktop/PROCESSED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### --- Start adaptation from Arezoo's notebook ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subject IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_name_list = []\n",
    "\n",
    "for x1 in images:\n",
    "    im_name_list.append(x1)\n",
    "im_name_list\n",
    "\n",
    "# im_name_list : list of image names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking into first image\n",
    "first_im = images['OAS30001']\n",
    "first = first_im['image']\n",
    "first.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "balancing_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "\n",
    "balancing_weights = dict(enumerate(balancing_weights))\n",
    "balancing_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_modified = np.reshape(first, (120, 160, 120, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_modified.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (9, 120, 160, 120, 1))\n",
    "x_test = np.reshape(x_test, (2, 120, 160, 120, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.reshape(y_train, (9, 1))\n",
    "y_test = np.reshape(y_test, (2, 1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv3D, MaxPool3D, Dropout, \\\n",
    "    Flatten, Conv3DTranspose, UpSampling3D, Reshape\n",
    "from tensorflow.keras.layers import Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(120, 160, 120 , 1), name='Input')\n",
    "\n",
    "print('\\ninput shape:', input_img.shape)\n",
    "\n",
    "x = Conv3D(10, (3,3,3), activation='relu', padding='same', name='Convolution_1')(input_img)\n",
    "x = MaxPool3D((2,2,2), name='MaxPooling_1')(x)\n",
    "x = Dropout(0.8, name='DropOut_1')(x)\n",
    "\n",
    "x = Conv3D(10, (3,3,3), activation='relu', padding='same', name='Convolution_2')(x)\n",
    "x = MaxPool3D((2,2,2), name='MaxPooling_2')(x)\n",
    "x = Dropout(0.8, name='DropOut_2')(x)\n",
    "\n",
    "x = Conv3D(10, (3,3,3), activation='relu', padding='same', name='Convolution_3')(x)\n",
    "x = MaxPool3D((2,2,2), name='MaxPooling_3')(x)\n",
    "x = Dropout(0.8, name='DropOut_3')(x)\n",
    "\n",
    "conv_shape = K.int_shape(x)\n",
    "\n",
    "print('shape after convolutions:', conv_shape)\n",
    "\n",
    "# x = Flatten(name='Flatten')(x)\n",
    "\n",
    "# x = Dense(32, activation='selu', name='SELU_1')(x)\n",
    "# x = Dense(16, activation='selu', name='SELU_2')(x)\n",
    "\n",
    "# encoded = Dense(2, name='Encoded')(x)\n",
    "\n",
    "# encoded_shape = K.int_shape(encoded)\n",
    "\n",
    "# print('final encoded shape:', encoded_shape, '\\n')\n",
    "\n",
    "encoded = x\n",
    "encoded_shape = K.int_shape(encoded)\n",
    "\n",
    "\n",
    "encoder = Model(input_img, encoded, name='CAE_encoder')\n",
    "encoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model only: post-encoder\n",
    "\n",
    "# x = Flatten(name='Flatten')(x)\n",
    "\n",
    "# x = Dense(32, activation='selu', name='SELU_1')(x)\n",
    "# x = Dense(16, activation='selu', name='SELU_2')(x)\n",
    "\n",
    "# encoded = Dense(2, name='Encoded')(x)\n",
    "\n",
    "# encoded_shape = K.int_shape(encoded)\n",
    "\n",
    "# print('final encoded shape:', encoded_shape, '\\n')\n",
    "\n",
    "# encoder = Model(input_img, encoded, name='CAE_encoder')\n",
    "# encoder.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CAE Encoder layers\n",
    "\n",
    "Input\n",
    "\n",
    "    120 x 160 x 120 x 1\n",
    "\n",
    "Convolutional layer (10 channels, 3x3x3 kernel, ReLU activation, padding)\n",
    "\n",
    "    120 x 160 x 120 x 10\n",
    "\n",
    "Maxpooling downsample (2x2x2 kernel)\n",
    "\n",
    "    60 x 80 x 60 x 10\n",
    "\n",
    "Dropout (80% of nodes set to 0)\n",
    "\n",
    "    60 x 80 x 60 x 10\n",
    "\n",
    "Convolutional layer (10 channels, 3x3x3 kernel, ReLU activation, padding)\n",
    "\n",
    "    60 x 80 x 60 x 10\n",
    "\n",
    "Maxpooling downsample (2x2x2 kernel)\n",
    "\n",
    "    30 x 40 x 30 x 10\n",
    "\n",
    "Dropout (80% of nodes set to 0)\n",
    "\n",
    "    30 x 40 x 30 x 10\n",
    "\n",
    "Convolutional layer (10 channels, 3x3x3 kernel, ReLU activation, padding)\n",
    "\n",
    "    30 x 40 x 30 x 10\n",
    "\n",
    "Maxpooling downsample (2x2x2 kernel)\n",
    "\n",
    "    15 x 20 x 15 x 10\n",
    "\n",
    "Dropout (80% of nodes set to 0)\n",
    "\n",
    "    15 x 20 x 15 x 10\n",
    "    \n",
    "\n",
    "<!-- ###### In the Classifier only:\n",
    "\n",
    "Flatten\n",
    "\n",
    "    45000\n",
    "\n",
    "Dense layer (32 channels, SELU activation)\n",
    "\n",
    "    32\n",
    "\n",
    "Dense layer (16 channels, SELU activation)\n",
    "\n",
    "    16\n",
    "\n",
    "Dense layer (2 possible outputs)\n",
    "\n",
    "    2\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = Input(shape=(encoded_shape[1],encoded_shape[2], encoded_shape[3], encoded_shape[4],), name='Encoded')\n",
    "\n",
    "x = encoded_input\n",
    "\n",
    "print('\\ninput shape:', encoded_input.shape)\n",
    "\n",
    "# x = Dense(16, activation='selu', name='SELU_2')(encoded_input)\n",
    "# x = Dense(32, activation='selu', name='SELU_1')(x)\n",
    "# x = Dense(np.prod(conv_shape[1:]), name='Product')(x)\n",
    "# x = Reshape((conv_shape[1], conv_shape[2], conv_shape[3], conv_shape[4]), name='Reshape')(x)\n",
    "\n",
    "# print('shape after reshape:', K.int_shape(x))\n",
    "\n",
    "x = Conv3DTranspose(10, (3,3,3), activation='relu', padding='same', name='Convolution_3')(x)\n",
    "x = UpSampling3D((2,2,2), name='UpSampling_3')(x)\n",
    "x = Dropout(0.8, name='DropOut_3')(x)\n",
    "\n",
    "x = Conv3DTranspose(10, (3,3,3), activation='relu', padding='same', name='Convolution_2')(x)\n",
    "x = UpSampling3D((2,2,2), name='UpSampling_2')(x)\n",
    "x = Dropout(0.8, name='DropOut_2')(x)\n",
    "\n",
    "x = Conv3DTranspose(10, (3,3,3), activation='relu', padding='same', name='Convolution_1')(x)\n",
    "x = UpSampling3D((2,2,2), name='UpSampling_1')(x)\n",
    "x = Dropout(0.8, name='DropOut_1')(x)\n",
    "\n",
    "# decoded = Dense(1, activation='sigmoid', name='Output')(x)\n",
    "decoded = Conv3DTranspose(1, (3,3,3), activation='sigmoid', padding='same', name='Output')(x)\n",
    "\n",
    "print('final decoded shape:', K.int_shape(decoded), '\\n')\n",
    "\n",
    "decoder = Model(encoded_input, decoded, name='CAE_decoder')\n",
    "decoder.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    " ###### In the Classifier only:\n",
    "\n",
    "\n",
    "Input\n",
    "\n",
    "    2\n",
    "\n",
    "Dense layer (16 channels, SELU activation)\n",
    "\n",
    "    16\n",
    "\n",
    "Dense layer (32 channels, SELU activation)\n",
    "\n",
    "    32\n",
    "\n",
    "Dense layer (product of encoded_shape dimensions 15x20x15x10)\n",
    "\n",
    "    45000\n",
    "\n",
    "Reshape to encoded_shape dimensions\n",
    "\n",
    "    15 x 20 x 15 x 10 \n",
    "    \n",
    "     -->\n",
    "##### CAE Decoder layers\n",
    "\n",
    "Input\n",
    "\n",
    "    15 x 20 x 15 x 10\n",
    "\n",
    "Convolutional transpose layer (10 channels, 3x3x3 kernel, ReLU activation, padding)\n",
    "\n",
    "    15 x 20 x 15 x 10\n",
    "\n",
    "Upsampling (2x2x2 kernel)\n",
    "\n",
    "    30 x 40 x 30 x 10\n",
    "\n",
    "Dropout (80% of nodes set to 0)\n",
    "\n",
    "    30x 40 x 30 x 10\n",
    "\n",
    "Convolutional transpose layer (10 channels, 3x3x3 kernel, ReLU activation, padding)\n",
    "\n",
    "    30 x 40 x 30 x 10\n",
    "\n",
    "Upsampling (2x2x2 kernel)\n",
    "\n",
    "    60 x 80 x 60 x 10\n",
    "\n",
    "Dropout (80% of nodes set to 0)\n",
    "\n",
    "    60 x 80 x 60 x 10\n",
    "\n",
    "Convolutional transpose layer (10 channels, 3x3x3 kernel, ReLU activation, padding)\n",
    "\n",
    "    60 x 80 x 60 x 10\n",
    "\n",
    "Upsampling (2x2x2 kernel)\n",
    "\n",
    "    120 x 160 x 120 x 10\n",
    "\n",
    "Dropout (80% of nodes set to 0)\n",
    "\n",
    "    120 x 160 x 120 x 10\n",
    "\n",
    "Convolutional transpose layer (1 channel, 3x3x3 kernel, sigmoid activation, padding)\n",
    "\n",
    "    120 x 160 x 120 x 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(input_img, decoder(encoder(input_img)), name='CAE_autoencoder')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "hist = autoencoder.fit(x_train, x_train, epochs=1, verbose=1, validation_data=(x_test, x_test)) # epochs=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### --- End adaption from Arezoo's notebook ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICAE Inception Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv3D, MaxPool3D, Dropout, Flatten, \\\n",
    "    Conv3DTranspose, UpSampling3D, Reshape\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.regularizers import l1_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_module(input, filter_operation):\n",
    "    \"\"\"\n",
    "    filter_operation is a list of inception operations for:\n",
    "        1x1x1 kernel, \n",
    "        1x1x1 then 3x3x3 kernels, \n",
    "        1x1x1 then 3x3x3 then 3x3x3 kernels, \n",
    "        and 3x3x3 maxpooling then 1x1x1 kernels, \n",
    "            respectively.\n",
    "    \"\"\"\n",
    "    \n",
    "    # According to Nature paper (Oh et. al.), l1 and l2 values of 10e-4 performed the best \n",
    "    #   from experimentation with 0.01, 0.001, 0.0001, 0.00001 values.\n",
    "    l1_value = 10e-4\n",
    "    l2_value = 10e-4\n",
    "\n",
    "    # Branch A\n",
    "    branch_1x1x1 = Conv3D(filter_operation[0], kernel_size=(1, 1, 1), activation='relu', \\\n",
    "                          padding='same', kernel_regularizer=l1_l2(l1_value, l2_value), \\\n",
    "                          name='Branch_A_1x1x1')(input)\n",
    "\n",
    "    # Branch B\n",
    "    branch_3x3x3_initial = Conv3D(filter_operation[1], kernel_size=(1, 1, 1), activation='relu', \\\n",
    "                                  padding='same', kernel_regularizer=l1_l2(l1_value, l2_value), \\\n",
    "                                  name='Branch_B_1x1x1')(input)\n",
    "    branch_3x3x3 = Conv3D(filter_operation[1], kernel_size=(3, 3, 3), activation='relu', \\\n",
    "                          padding='same', kernel_regularizer=l1_l2(l1_value, l2_value), \\\n",
    "                          name='Branch_B_3x3x3')(branch_3x3x3_initial)\n",
    "\n",
    "    # Branch C\n",
    "    branch_double_3x3x3_initial = Conv3D(filter_operation[2], kernel_size=(1, 1, 1), \\\n",
    "                                         activation='relu', padding='same', \\\n",
    "                                         kernel_regularizer=l1_l2(l1_value, l2_value), \\\n",
    "                                         name='Branch_C_1x1x1')(input)\n",
    "    branch_double_3x3x3_middle = Conv3D(filter_operation[2], kernel_size=(3, 3, 3), \\\n",
    "                                        activation='relu', padding='same', \\\n",
    "                                        kernel_regularizer=l1_l2(l1_value, l2_value), \\\n",
    "                                        name='Branch_C_1st_3x3x3')(branch_double_3x3x3_initial)\n",
    "    branch_double_3x3x3 = Conv3D(filter_operation[2], kernel_size=(3, 3, 3), activation='relu', \\\n",
    "                                 padding='same', kernel_regularizer=l1_l2(l1_value, l2_value), \\\n",
    "                                 name='Branch_C_2nd_3x3x3')(branch_double_3x3x3_middle)\n",
    "\n",
    "    # Branch D\n",
    "    branch_maxpool_3x3x3_initial = MaxPool3D(pool_size=(3, 3, 3), strides=(1, 1, 1), \\\n",
    "                                             padding='same', name='Branch_D_3x3x3_maxpool')(input)\n",
    "    branch_maxpool_3x3x3 = Conv3D(filter_operation[3], kernel_size=(1, 1, 1), activation='relu', \\\n",
    "                                  padding='same', kernel_regularizer=l1_l2(l1_value, l2_value), \\\n",
    "                                  name='Branch_D_1x1x1')(branch_maxpool_3x3x3_initial)\n",
    "    \n",
    "    # Merge branches\n",
    "    modules = [branch_1x1x1, branch_3x3x3, branch_double_3x3x3, branch_maxpool_3x3x3]\n",
    "    merged_module = concatenate(modules, name='Inception_Merged') # axis=-1\n",
    "    \n",
    "    return merged_module\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape= (120, 160, 120 , 1), name='Input')\n",
    "\n",
    "print('\\ninput shape:', input_img.shape)\n",
    "\n",
    "x = Conv3D(10, (3,3,3), activation='relu', padding='same', name='Convolution_1')(input_img)\n",
    "x = MaxPool3D((2,2,2), name='MaxPooling_1')(x)\n",
    "x = Dropout(0.8, name='DropOut_1')(x)\n",
    "\n",
    "x = Conv3D(10, (3,3,3), activation='relu', padding='same', name='Convolution_2')(x)\n",
    "x = MaxPool3D((2,2,2), name='MaxPooling_2')(x)\n",
    "x = Dropout(0.8, name='DropOut_2')(x)\n",
    "\n",
    "# filter_operations = [40, 40, 40, 40]\n",
    "filter_operations = [10, 10, 10, 10]\n",
    "\n",
    "x = inception_module(x, filter_operations)\n",
    "x = MaxPool3D((2,2,2), name='Inception_MaxPooling')(x)\n",
    "encoded = Dropout(0.8, name='DropOut_3')(x)\n",
    "\n",
    "# print('shape after inception:', K.int_shape(x))\n",
    "\n",
    "# ## for model only\n",
    "# encoded = Flatten(name='Flatten')(encoded)\n",
    "# encoded = Dense(1, activation='sigmoid', name='Prediction')(encoded)\n",
    "# ##\n",
    "\n",
    "encoder = Model(input_img, encoded, name='ICAE_encoder')\n",
    "\n",
    "encoded_shape = K.int_shape(encoded)\n",
    "print('final encoded shape:', encoded_shape, '\\n')\n",
    "\n",
    "encoder.summary(positions=[.35, .64, .71, 1.]) # adjusts print settings to minimize truncation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ICAE Encoder layers\n",
    "\n",
    "Input\n",
    "\n",
    "    120 x 160 x 120 x 1\n",
    "\n",
    "Convolutional layer (10 channels, 3x3x3 kernel, ReLU activation, padding)\n",
    "\n",
    "    120 x 160 x 120 x 10\n",
    "\n",
    "Maxpooling downsample (2x2x2 kernel)\n",
    "\n",
    "    60 x 80 x 60 x 10\n",
    "\n",
    "Convolutional layer (10 channels, 3x3x3 kernel, ReLU activation, padding)\n",
    "\n",
    "    60 x 80 x 60 x 10\n",
    "\n",
    "Maxpooling downsample (2x2x2 kernel)\n",
    "\n",
    "    30 x 40 x 30 x 10\n",
    "\n",
    "Dropout (80% of nodes set to 0)\n",
    "\n",
    "    30 x 40 x 30 x 10\n",
    "\n",
    "Inception branched layers\n",
    "\n",
    "    A: 30 x 40 x 30 x 10\n",
    "\n",
    "    B: 30 x 40 x 30 x 10  ->  30 x 40 x 30 x 10\n",
    "\n",
    "    C: 30 x 40 x 30 x 10  ->  30 x 40 x 30 x 10  ->  30 x 40 x 30 x 10\n",
    "\n",
    "    D: 30 x 40 x 30 x 10  ->  30 x 40 x 30 x 10\n",
    "\n",
    "Inception merged layer\n",
    "\n",
    "    30 x 40 x 30 x 40\n",
    "\n",
    "Maxpooling downsample (2x2x2 kernel)\n",
    "\n",
    "    15 x 20 x 15 x 40\n",
    "\n",
    "Dense layer (2 possible outputs, SoftMax activation)\n",
    "\n",
    "    15 x 20 x 15 x 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input= Input(shape=(encoded_shape[1],encoded_shape[2], \\\n",
    "                            encoded_shape[3], encoded_shape[4],), name='Input')\n",
    "\n",
    "print('\\ninput shape:', encoded_input.shape)\n",
    "\n",
    "# filter_operations = [40, 40, 40, 40]\n",
    "# filter_operations = [10, 10, 10, 10]\n",
    "combined_dim = sum(filter_operations) # taken from encoder\n",
    "\n",
    "# x = Dense(combined_dim, activation='selu', name='SELU')(encoded_input)\n",
    "\n",
    "x = encoded_input\n",
    "\n",
    "x = inception_module(x, filter_operations)\n",
    "\n",
    "# print('shape after inception:', K.int_shape(x))\n",
    "\n",
    "x = UpSampling3D((2,2,2), name='Inception_Upsampling')(x)\n",
    "x = Dropout(0.8, name='DropOut_3')(x)\n",
    "\n",
    "x = Conv3DTranspose(10, (3,3,3), activation='relu', padding='same', name='Convolution_2')(x)\n",
    "x = UpSampling3D((2,2,2), name='Upsampling_2')(x)\n",
    "x = Dropout(0.8, name='DropOut_2')(x)\n",
    "\n",
    "x = Conv3DTranspose(10, (3,3,3), activation='relu', padding='same', name='Convolution_1')(x)\n",
    "x = UpSampling3D((2,2,2), name='Upsampling_1')(x)\n",
    "x = Dropout(0.8, name='DropOut_1')(x)\n",
    "\n",
    "# decoded = Dense(1, activation='sigmoid', name='Output')(x)\n",
    "decoded = Conv3DTranspose(1, (3,3,3), activation='sigmoid', padding='same', name='Output')(x)\n",
    "\n",
    "print('final decoded shape:', K.int_shape(decoded), '\\n')\n",
    "\n",
    "decoder = Model(encoded_input, decoded, name='ICAE_decoder')\n",
    "decoder.summary(positions=[.35, .64, .71, 1.])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ICAE Decoder layers\n",
    "\n",
    "Input\n",
    "\n",
    "    15 x 20 x 15 x 10\n",
    "\n",
    "Inception branched layers\n",
    "\n",
    "    A: 15 x 20 x 15 x 10\n",
    "\n",
    "    B: 15 x 20 x 15 x 10  ->  15 x 20 x 15 x 10\n",
    "\n",
    "    C: 15 x 20 x 15 x 10  ->  15 x 20 x 15 x 10  ->  15 x 20 x 15 x 10\n",
    "\n",
    "    D: 15 x 20 x 15 x 10  ->  15 x 20 x 15 x 10\n",
    "\n",
    "Upsampling (2x2x2 kernel)\n",
    "\n",
    "    30 x 40 x 30 x 10\n",
    "\n",
    "Dropout (80% of nodes set to 0)\n",
    "\n",
    "    30x 40 x 30 x 10\n",
    "\n",
    "Convolutional transpose layer (10 channels, 3x3x3 kernel, ReLU activation, padding)\n",
    "\n",
    "    30 x 40 x 30 x 10\n",
    "\n",
    "Upsampling (2x2x2 kernel)\n",
    "\n",
    "    60 x 80 x 60 x 10\n",
    "\n",
    "Dropout (80% of nodes set to 0)\n",
    "\n",
    "    60 x 80 x 60 x 10\n",
    "\n",
    "Convolutional transpose layer (10 channels, 3x3x3 kernel, ReLU activation, padding)\n",
    "\n",
    "    60 x 80 x 60 x 10\n",
    "\n",
    "Upsampling (2x2x2 kernel)\n",
    "\n",
    "    120 x 160 x 120 x 10\n",
    "\n",
    "Dropout (80% of nodes set to 0)\n",
    "\n",
    "    120 x 160 x 120 x 10\n",
    "\n",
    "Convolutional transpose layer (1 channel, 3x3x3 kernel, sigmoid activation, padding)\n",
    "\n",
    "    120 x 160 x 120 x 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICAE_autoencoder = Model(input_img, decoder(encoder(input_img)), name='ICAE_autoencoder')\n",
    "ICAE_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICAE_autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "ICAE_hist = ICAE_autoencoder.fit(x_train, x_train, epochs=1, verbose=1, \\\n",
    "                                 validation_data=(x_test, x_test)) # epochs=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
